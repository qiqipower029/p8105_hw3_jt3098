---
title: "p8105_hw3_jt3098"
author: "Jieqi Tu (jt3098)"
date: "10/5/2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(hexbin)
library(ggridges)
library(patchwork)
```

```{r Plots setting, include=FALSE}
knitr::opts_chunk$set(
  fig.width = 10,
  fig.asp = .6,
  out.width = "100%"
)
```


## Problem 1
```{r Importing data}
# Importing data from guthub website
devtools::install_github("p8105/p8105.datasets")
library(p8105.datasets)
data(brfss_smart2010, package = "p8105.datasets")
```


```{r Data Tidying}
brfss_smart2010 = 
  brfss_smart2010 %>%
  janitor::clean_names() %>% # Change upper-case letter to lower-case letter
  filter(topic == "Overall Health") %>% # Focus on the topic of Overall Health
  filter(response %in% c("Excellent", "Very good", "Good", "Fair", "Poor"))
```


```{r convertion of "response"}
brfss_smart2010$response = factor(brfss_smart2010$response, 
                                  levels = c("Excellent", "Very good", "Good", "Fair", "Poor"))
```

```{r Questions for problem 1}
brfss_smart2002 = 
  brfss_smart2010 %>%
  filter(year == "2002") %>%
  group_by(locationabbr) %>%
  summarise(n_observed_location = n_distinct(geo_location)) %>%
  filter(n_observed_location == 7)
```

In 2002, there 3 states observed in 7 locations. They are Connecticut (CT), Florida (FL), and North Carolina (NC).

```{r Spaghetti plot}
brfss_smart2010 %>%
  group_by(locationabbr, year) %>%
  summarise(n_observation = sum(sample_size)) %>%
  ggplot(aes(x = year, y = n_observation, color = locationabbr)) +
  geom_line(alpha = 0.5) +
  labs(
    title = "Observation Times of Each States from 2002 to 2010",
    y = "Observation Times",
    x = "Year"
  ) + 
  theme_bw()
```

Comments: From the plot we could know that, most states were not observed greater than 10000 times. From 2002 to 2010, Florida has the highest observation times in 2007. 

```{r Table for "Excellent" proportion sd and mean}
brfss_prop_020610 = 
  brfss_smart2010 %>%
  filter(year %in% c("2002", "2006", "2010"), response == "Excellent", locationabbr == "NY") %>%
  group_by(year) %>%
  summarise(mean = mean(data_value), standard_deviation = sd(data_value))
brfss_prop_020610
```

Comments: In 2002, the mean proportion of "Excellent" response is the highest (24.0), with the highest standard deviation. In 2010, the standard deviation of mean proportion of "Excellent" reponse is the lowest (3.57). However, in 2006, the mean proportion of "Excellent" response is the lowest (22.5).

```{r Five-panel Plot}
brfss_prop_each = 
  brfss_smart2010 %>%
  group_by(year, locationabbr, response) %>%
  summarise(average_proportion = mean(data_value, rm.na = TRUE))

ggplot(brfss_prop_each, aes(x = year, y = average_proportion, color = locationabbr)) +
  geom_line(alpha = 0.5) +
  facet_grid( ~response) +
  labs(
    title = "Average Proportion for Each Response in Every States and Location from 2002 to 2010",
    x = "Year",
    y = "Average Proportion"
  ) +
  scale_x_continuous(
    breaks = c(2004, 2008),
    labels = c("2004", "2008")) +
  theme_bw()
```

Comments: In general, the largest proportion of response is "Very good", and "Good" is the second large proportion of response. "Excellent" is the third, while "Fair" and "Poor" are the fourth and fifth.

## Problem 2
```{r Importing data for problem 2}
data("instacart", package = "p8105.datasets")
```

```{r intstacart questions}
# Calculate the number of aisles
nrow(instacart)
ncol(instacart)
n_aisle = 
  instacart %>%
  distinct(aisle) %>%
  nrow()

# Count the most ordered aisle
count_(instacart, 'aisle', sort = TRUE)
```

About this dataset:

* This dataset has 15 columns and 1384617 rows.
* This instacart dataset has 15 variables in total. It includes user ID, product ID, order number, order hour of day, days since prior order, etc.
* The mean of order hour of day is `r mean(instacart$order_hour_of_day, rm.na = TRUE)`, and the standard deviation of order hour of day is `r sd(instacart$order_hour_of_day)`.
* The mean of days since prior order is `r mean(instacart$days_since_prior_order, rm.na = TRUE)`, and the standard deviation of days since prior order is `r sd(instacart$days_since_prior_order)`.
* The number of aisle is `r n_aisle`, and the aisle that most items ordered from is fresh vegetables with 150609 orders.

```{r plots of number of items ordered in each aisle}
instacart %>%
  group_by(aisle_id, department) %>%
  summarise(total_aisle = n()) %>%
  ggplot(aes(x = aisle_id, y = total_aisle, color = department)) +
  geom_point(alpha = 0.4) +
  labs(
    x = "Aisle ID",
    y = "Number of Items Ordered",
    title = "Number of Items Ordered in Each Aisle",
    caption = "The aisle name corresponded to the aisle ID can be found in Table `instacart`."

  ) +
  scale_x_continuous(
    breaks = c(0, 25, 50, 75, 100, 125, 150),
    labels = c("0", "25", "50", "75", "100", "125", "150")
  ) +
  theme_bw() 
```

Comments: Mostly, the number of items ordered in each aisle is less than 100000. However, there are still two aisle that has number of items ordered greater than 150000. They are #24 aisle "fresh fruits" and #83 aisle "fresh vegetables", both of which are from produce department.

```{r Most popular item aisles}
pop_items_aisle = 
  instacart %>%
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle, product_name) %>%
  summarise(number_of_items = n()) %>%
  filter(number_of_items == max(number_of_items))
```

Comments: The most popular item in aisle "baking ingredients" is Light Brown Sugar with 499 orders; while the Snack Stick Chicken & Rice Recipe Dog Treats is ordered most in "dog food care" with 30 times, and Organic Baby Spinach is ordered most in "packaged vegetables fruits" with 9794 times.

```{r Mean hour of the day}
mean_hour_of_day_table = 
  instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarise(mean_hour_of_a_day = mean(order_hour_of_day)) %>%
  spread(key = order_dow, value = mean_hour_of_a_day) %>%
  `colnames<-`(c("Product Name", "Sun", "Mon", "Tue", "Wed", "Thr", "Fri", "Sat"))
```

Comments: For both Coffee Ice Crean and Pink Lady Apples, Sunday and Saturday have the highest mean order hour within this week. Generally speaking, Coffee Ice Cream has a longer order hour than Pink Lady Appples on each day of this week.

## Problem 3
```{r Importing data for problem 3}
data("ny_noaa")
n_row = nrow(ny_noaa)
n_col = ncol(ny_noaa)
View(ny_noaa)
ny_noaa = 
  ny_noaa %>%
  janitor::clean_names() %>%
  separate(col = date, into = c("year", "month", "day"), sep = "-") %>%
  mutate(
    year = as.double(year),
    tmax = as.double(tmax) / 10,
    tmin = as.double(tmin) / 10,
    prcp = prcp / 10
  )

n_NA = sapply(ny_noaa[1:9], function(x) sum(length(which(is.na(x)))))
```

Description of the dataset:

* The size of the dataset is that it has `r n_row` rows and `r n_col` columns.
* It provides information about record ID, date of observation, precipitation, snowfalls, snow depth, the minimum temperature and the maximum temperature of day, etc.
* The average precipitation from 1981 to 2010 is `r mean(prcp)`, and the standard deviation for the precipitation from 1981 to 2010 is `r sd(prcp)`.
* From the dataset, we could know that, there is no NA in id and dates. There are 145838 NAs in precipitation, 381221 in snow, 591786 in snwd, 1134358 in tman and 1134420 in tmin. If the distribution of NA is concentrated in one or two variables, the missing data could be an issue; if the NAs are distributed relatively evenly in many variables, the missing data might be an issue.

```{r observed most temperature in snowfall}
ny_noaa %>%
  filter(!is.na(snow)) %>%
  group_by(snow) %>%
    summarise(n_value = n()) %>%
    filter(n_value == max(n_value))
```

* The most observed temperature for snawfall is 0, because 0 is the melting point for ice. It would lasts for a longer time than other temperatures.

```{r two-panel plot for tmax in Janurary and July}
ny_noaa %>%
  filter(month %in% c("01", "07")) %>%
  group_by(year, month, id) %>%
  summarise(average_tmax = mean(tmax, na.rm = TRUE)) %>%
  ggplot(aes(x = year, y = average_tmax, color = id)) +
  geom_line(alpha = .4) +
  labs(
    title = "Average Max Temperature of Janurary and July",
    x = "Year",
    y = "Average Max Temperature (ÂºC)"
  ) +
  scale_x_continuous(
    breaks = c(1981, 1990, 2000, 2010),
    labels = c("1981", "1990", "2000", "2010")
  ) +
  geom_point(alpha = 0.4) +
  facet_grid(~month) +
  theme_bw() +
  theme(legend.position = "none")
```

Comments:
From the two-panel plots we could know the data recorded in each station about the everage maximum temeperature in January and July. We could use spaghetti strcture to show the data, because we need to provide year, average tmax for each station. If we use boxplot, we could be able to see outliers but it could not give us information among stations. Therefore, speghetti structure is the best choice (for me). We could also see the outliers that lies out of the clusters.

```{r two-panel plot for tmax and tmin}
temp_dist = 
  ny_noaa %>%
  filter(!is.na(tmin), !is.na(tmax)) %>%
  ggplot(aes(x = tmin, y = tmax)) +
  geom_hex() +
  labs(
    title = "Distribution of tmin and tmax from 1981 to 2010"
  ) +
  theme_bw() 

snow_dist = 
  ny_noaa %>%
  filter(!is.na(snow)) %>%
  filter(snow < 100 & snow > 0) %>%
  mutate(year = as.factor(year)) %>%
  ggplot(aes(x = snow, y = year)) +
  geom_density_ridges() +
  labs(
    title = "Distribution of Snowfall > 0 and Snowfall < 100",
    x = "Snowfall",
    y = "Year"
  ) +
  theme_bw()

snow_dist + temp_dist
```

Comments: In general, the distribution of snowfall value in each year has a similar shape with very slight difference. However, in the distribution of tmax and tmin, there are many outliers. The most observed value for tmax is approximately 0 to 30 ÂºC, while the most observed value for tmin is -15ÂºC to 15ÂºC.
