---
title: "p8105_hw3_jt3098"
author: "Jieqi Tu (jt3098)"
date: "10/5/2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(hexbin)
```

```{r Plots setting, include=FALSE}
knitr::opts_chunk$set(
  fig.width = 8,
  fig.asp = .6,
  out.width = "100%"
)
```


## Problem 1
```{r Importing data}
# Importing data from guthub website
devtools::install_github("p8105/p8105.datasets")
library(p8105.datasets)
data(brfss_smart2010, package = "p8105.datasets")
```


```{r Data Tidying}
brfss_smart2010 = 
  brfss_smart2010 %>%
  janitor::clean_names() %>% # Change upper-case letter to lower-case letter
  filter(topic == "Overall Health") %>% # Focus on the topic of Overall Health
  filter(response %in% c("Excellent", "Very good", "Good", "Fair", "Poor"))
```


```{r convertion of "response"}
brfss_smart2010$response = factor(brfss_smart2010$response, 
                                  levels = c("Excellent", "Very good", "Good", "Fair", "Poor"))
```

```{r Questions for problem 1}
brfss_smart2002 = 
  brfss_smart2010 %>%
  filter(year == "2002") %>%
  group_by(locationabbr) %>%
  summarise(n_observed_location = n_distinct(geo_location)) %>%
  filter(n_observed_location == 7)
```

In 2002, there 3 states observed in 7 locations. They are Connecticut (CT), Florida (FL), and North Carolina (NC).

```{r Spaghetti plot}
brfss_smart2010 %>%
  group_by(locationabbr, year) %>%
  summarise(n_observation = sum(sample_size)) %>%
  ggplot(aes(x = year, y = n_observation, color = locationabbr)) +
  geom_line(alpha = 0.5) +
  labs(
    title = "Observation Times of Each States from 2002 to 2010",
    y = "Observation Times",
    x = "Year"
  ) + 
  theme_bw()
```

Comments:

```{r Table for "Excellent" proportion sd and mean}
brfss_prop_020610 = 
  brfss_smart2010 %>%
  filter(year %in% c("2002", "2006", "2010"), response == "Excellent", locationabbr == "NY") %>%
  group_by(year) %>%
  summarise(mean = mean(data_value), standard_deviation = sd(data_value))
```

Comments:

```{r Five-panel Plot}
brfss_prop_each = 
  brfss_smart2010 %>%
  group_by(year, locationabbr, response) %>%
  summarise(average_proportion = mean(data_value, rm.na = TRUE))

ggplot(brfss_prop_each, aes(x = year, y = average_proportion, color = locationabbr)) +
  geom_line(alpha = 0.5) +
  facet_grid( ~response) +
  labs(
    title = "Average Proportion for Each Response in Every States and Location from 2002 to 2010",
    x = "Year",
    y = "Average Proportion"
  ) +
  scale_x_continuous(
    breaks = c(2004, 2008),
    labels = c("2004", "2008")) +
  theme_bw()
```

Comments:

## Problem 2
```{r Importing data for problem 2}
data("instacart", package = "p8105.datasets")
```

```{r intstacart questions}
# Calculate the number of aisles
n_aisle = 
  instacart %>%
  distinct(aisle) %>%
  nrow()

# Count the most ordered aisle
count_(instacart, 'aisle', sort = TRUE)
```

About this dataset:

* This instacart dataset has 15 variables in total. It includes user ID, product ID, order number, order hour of day, days since prior order, etc.
* The mean of order hour of day is `r mean(instacart$order_hour_of_day, rm.na = TRUE)`, and the standard deviation of order hour of day is `r sd(instacart$order_hour_of_day)`.
* The mean of days since prior order is `r mean(instacart$days_since_prior_order, rm.na = TRUE)`, and the standard deviation of days since prior order is `r sd(instacart$days_since_prior_order)`.
* The number of aisle is `r n_aisle`, and the aisle that most items ordered from is fresh vegetables with 150609 orders.

```{r plots of number of items ordered in each aisle}
instacart %>%
  group_by(aisle_id) %>%
  summarise(total_aisle = n()) %>%
  ggplot(aes(x = aisle_id, y = total_aisle)) +
  geom_point(alpha = 0.4) +
  labs(
    x = "Aisle ID",
    y = "Number of Items Ordered",
    title = "Number of Items Ordered in Each Aisle",
    caption = "The aisle name corresponded to the aisle ID can be found in Table `instacart`."

  ) +
  scale_x_continuous(
    breaks = c(0, 25, 50, 75, 100, 125, 150),
    labels = c("0", "25", "50", "75", "100", "125", "150")
  ) +
  theme_bw()
```

Comments:

```{r Most popular item aisles}
pop_items_aisle = 
  instacart %>%
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle, product_name) %>%
  summarise(number_of_items = n()) %>%
  filter(number_of_items == max(number_of_items))
```

Comments: The most popular item in aisle "baking ingredients" is Light Brown Sugar with 499 orders; while the Snack Stick Chicken & Rice Recipe Dog Treats is ordered most in "dog food care" with 30 times, and Organic Baby Spinach is ordered most in "packaged vegetables fruits" with 9794 times.

```{r Mean hour of the day}
mean_hour_of_day_table = 
  instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarise(mean_hour_of_a_day = mean(order_hour_of_day)) %>%
  spread(key = order_dow, value = mean_hour_of_a_day) %>%
  `colnames<-`(c("Product Name", "Sun", "Mon", "Tue", "Wed", "Thr", "Fri", "Sat"))
```

Comments: For both Coffee Ice Crean and Pink Lady Apples, Sunday and Saturday have the highest mean order hour within this week. Generally speaking, Coffee Ice Cream has a longer order hour than Pink Lady Appples on each day of this week.

## Problem 3
```{r Importing data for problem 3}
data("ny_noaa")
n_row = nrow(ny_noaa)
n_col = ncol(ny_noaa)
View(ny_noaa)
ny_noaa = 
  ny_noaa %>%
  janitor::clean_names() %>%
  separate(col = date, into = c("year", "month", "day"), sep = "-") %>%
  mutate(
    year = as.double(year),
    tmax = as.double(tmax) / 10,
    tmin = as.double(tmin) / 10,
    prcp = prcp / 10
  )

n_NA = sapply(ny_noaa[1:9], function(x) sum(length(which(is.na(x)))))
```

Description of the dataset:

* The size of the dataset is that it has `r n_row` rows and `r n_col` columns.
* It provides information about record ID, date of observation, precipitation, snowfalls, snow depth, the minimum temperature and the maximum temperature of day, etc.
* The average precipitation from 1981 to 2010 is `r mean(prcp)`, and the standard deviation for the precipitation from 1981 to 2010 is `r sd(prcp)`.
* From the dataset, we could know that, there is no NA in id and dates. There are 145838 NAs in precipitation, 381221 in snow, 591786 in snwd, 1134358 in tman and 1134420 in tmin. If the distribution of NA is concentrated in one or two variables, the missing data could be an issue; if the NAs are distributed relatively evenly in many variables, the missing data might be an issue.

```{r observed most temperature in snowfall}
ny_noaa %>%
  filter(!is.na(snow)) %>%
  group_by(snow) %>%
    summarise(n_value = n()) %>%
    filter(n_value == max(n_value))
```

* The most observed temperature for snawfall is 0, because 0 is the melting point for ice. It would lasts for a longer time than other temperatures.

```{r two-panel plot for tmax in Janurary and July}
ny_noaa %>%
  filter(month %in% c("01", "07")) %>%
  group_by(year, month, id) %>%
  summarise(average_tmax = mean(tmax, na.rm = TRUE)) %>%
  ggplot(aes(x = year, y = average_tmax, color = id)) +
  geom_line(alpha = .4) +
  labs(
    title = "Average Max Temperature of Janurary and July",
    x = "Year",
    y = "Average Max Temperature (ÂºC)"
  ) +
  scale_x_continuous(
    breaks = c(1981, 1990, 2000, 2010),
    labels = c("1981", "1990", "2000", "2010")
  ) +
  geom_point(alpha = 0.4) +
  facet_grid(~month) +
  theme_bw() +
  theme(legend.position = "none")
```

```{r two-panel plot for tmax and tmin}

```

