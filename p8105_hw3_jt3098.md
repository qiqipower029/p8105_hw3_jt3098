p8105\_hw3\_jt3098
================
Jieqi Tu (jt3098)
10/5/2018

Problem 1
---------

``` r
# Importing data from guthub website
devtools::install_github("p8105/p8105.datasets")
```

    ## Skipping install of 'p8105.datasets' from a github remote, the SHA1 (21f5ad1c) has not changed since last install.
    ##   Use `force = TRUE` to force installation

``` r
library(p8105.datasets)
data(brfss_smart2010, package = "p8105.datasets")
```

``` r
brfss_smart2010 = 
  brfss_smart2010 %>%
  janitor::clean_names() %>% # Change upper-case letter to lower-case letter
  filter(topic == "Overall Health") %>% # Focus on the topic of Overall Health
  filter(response %in% c("Excellent", "Very good", "Good", "Fair", "Poor"))
```

``` r
brfss_smart2010$response = factor(brfss_smart2010$response, 
                                  levels = c("Excellent", "Very good", "Good", "Fair", "Poor"))
```

``` r
brfss_smart2002 = 
  brfss_smart2010 %>%
  filter(year == "2002") %>%
  group_by(locationabbr) %>%
  summarise(n_observed_location = n_distinct(geo_location)) %>%
  filter(n_observed_location == 7)
```

In 2002, there 3 states observed in 7 locations. They are Connecticut (CT), Florida (FL), and North Carolina (NC).

``` r
brfss_smart2010 %>%
  group_by(locationabbr, year) %>%
  summarise(n_observation = sum(sample_size)) %>%
  ggplot(aes(x = year, y = n_observation, color = locationabbr)) +
  geom_line(alpha = 0.5) +
  labs(
    title = "Observation Times of Each States from 2002 to 2010",
    y = "Observation Times",
    x = "Year"
  ) + 
  theme_bw()
```

<img src="p8105_hw3_jt3098_files/figure-markdown_github/Spaghetti plot-1.png" width="100%" />

Comments: From the plot we could know that, most states were not observed greater than 10000 times. From 2002 to 2010, Florida has the highest observation times in 2007.

``` r
brfss_prop_020610 = 
  brfss_smart2010 %>%
  filter(year %in% c("2002", "2006", "2010"), response == "Excellent", locationabbr == "NY") %>%
  group_by(year) %>%
  summarise(mean = mean(data_value), standard_deviation = sd(data_value))
brfss_prop_020610
```

    ## # A tibble: 3 x 3
    ##    year  mean standard_deviation
    ##   <int> <dbl>              <dbl>
    ## 1  2002  24.0               4.49
    ## 2  2006  22.5               4.00
    ## 3  2010  22.7               3.57

Comments: In 2002, the mean proportion of "Excellent" response is the highest (24.0), with the highest standard deviation. In 2010, the standard deviation of mean proportion of "Excellent" reponse is the lowest (3.57). However, in 2006, the mean proportion of "Excellent" response is the lowest (22.5).

``` r
brfss_prop_each = 
  brfss_smart2010 %>%
  group_by(year, locationabbr, response) %>%
  summarise(average_proportion = mean(data_value, rm.na = TRUE))

ggplot(brfss_prop_each, aes(x = year, y = average_proportion, color = locationabbr)) +
  geom_line(alpha = 0.5) +
  facet_grid( ~response) +
  labs(
    title = "Average Proportion for Each Response in Every States and Location from 2002 to 2010",
    x = "Year",
    y = "Average Proportion"
  ) +
  scale_x_continuous(
    breaks = c(2004, 2008),
    labels = c("2004", "2008")) +
  theme_bw()
```

    ## Warning: Removed 1 rows containing missing values (geom_path).

<img src="p8105_hw3_jt3098_files/figure-markdown_github/Five-panel Plot-1.png" width="100%" />

Comments: In general, the largest proportion of response is "Very good", and "Good" is the second large proportion of response. "Excellent" is the third, while "Fair" and "Poor" are the fourth and fifth.

Problem 2
---------

``` r
data("instacart", package = "p8105.datasets")
```

``` r
# Calculate the number of aisles
nrow(instacart)
```

    ## [1] 1384617

``` r
ncol(instacart)
```

    ## [1] 15

``` r
n_aisle = 
  instacart %>%
  distinct(aisle) %>%
  nrow()

# Count the most ordered aisle
count_(instacart, 'aisle', sort = TRUE)
```

    ## # A tibble: 134 x 2
    ##    aisle                              n
    ##    <chr>                          <int>
    ##  1 fresh vegetables              150609
    ##  2 fresh fruits                  150473
    ##  3 packaged vegetables fruits     78493
    ##  4 yogurt                         55240
    ##  5 packaged cheese                41699
    ##  6 water seltzer sparkling water  36617
    ##  7 milk                           32644
    ##  8 chips pretzels                 31269
    ##  9 soy lactosefree                26240
    ## 10 bread                          23635
    ## # ... with 124 more rows

About this dataset:

-   This dataset has 15 columns and 1384617 rows.
-   This instacart dataset has 15 variables in total. It includes user ID, product ID, order number, order hour of day, days since prior order, etc.
-   The mean of order hour of day is 13.5775922, and the standard deviation of order hour of day is 4.238458.
-   The mean of days since prior order is 17.0661259, and the standard deviation of days since prior order is 10.4264178.
-   The number of aisle is 134, and the aisle that most items ordered from is fresh vegetables with 150609 orders.

``` r
instacart %>%
  group_by(aisle_id, department) %>%
  summarise(total_aisle = n()) %>%
  ggplot(aes(x = aisle_id, y = total_aisle, color = department)) +
  geom_point(alpha = 0.4) +
  labs(
    x = "Aisle ID",
    y = "Number of Items Ordered",
    title = "Number of Items Ordered in Each Aisle",
    caption = "The aisle name corresponded to the aisle ID can be found in Table `instacart`."

  ) +
  scale_x_continuous(
    breaks = c(0, 25, 50, 75, 100, 125, 150),
    labels = c("0", "25", "50", "75", "100", "125", "150")
  ) +
  theme_bw() 
```

<img src="p8105_hw3_jt3098_files/figure-markdown_github/plots of number of items ordered in each aisle-1.png" width="100%" />

Comments: Mostly, the number of items ordered in each aisle is less than 100000. However, there are still two aisle that has number of items ordered greater than 150000. They are \#24 aisle "fresh fruits" and \#83 aisle "fresh vegetables", both of which are from produce department.

``` r
pop_items_aisle = 
  instacart %>%
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle, product_name) %>%
  summarise(number_of_items = n()) %>%
  filter(number_of_items == max(number_of_items))
pop_items_aisle
```

    ## # A tibble: 3 x 3
    ## # Groups:   aisle [3]
    ##   aisle                product_name                        number_of_items
    ##   <chr>                <chr>                                         <int>
    ## 1 baking ingredients   Light Brown Sugar                               499
    ## 2 dog food care        Snack Sticks Chicken & Rice Recipe…              30
    ## 3 packaged vegetables… Organic Baby Spinach                           9784

Comments: The most popular item in aisle "baking ingredients" is Light Brown Sugar with 499 orders; while the Snack Stick Chicken & Rice Recipe Dog Treats is ordered most in "dog food care" with 30 times, and Organic Baby Spinach is ordered most in "packaged vegetables fruits" with 9784 times.

``` r
mean_hour_of_day_table = 
  instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarise(mean_hour_of_a_day = mean(order_hour_of_day)) %>%
  spread(key = order_dow, value = mean_hour_of_a_day) %>%
  `colnames<-`(c("Product Name", "Sun", "Mon", "Tue", "Wed", "Thr", "Fri", "Sat"))
```

Comments: For both Coffee Ice Crean and Pink Lady Apples, Sunday and Saturday have the highest mean order hour within this week. Generally speaking, Coffee Ice Cream has a longer order hour than Pink Lady Appples on each day of this week.

Problem 3
---------

``` r
data("ny_noaa", package = "p8105.datasets")
n_row = nrow(ny_noaa)
n_col = ncol(ny_noaa)
ny_noaa = 
  ny_noaa %>%
  janitor::clean_names() %>%
  separate(col = date, into = c("year", "month", "day"), sep = "-") %>%
  mutate(
    year = as.double(year),
    tmax = as.double(tmax) / 10,
    tmin = as.double(tmin) / 10,
    prcp = prcp / 10
  )

n_NA = sapply(ny_noaa[1:9], function(x) sum(length(which(is.na(x)))))
```

Description of the dataset:

-   The size of the dataset is that it has 2595176 rows and 7 columns.
-   It provides information about record ID, date of observation, precipitation, snowfalls, snow depth, the minimum temperature and the maximum temperature of day, etc.
-   The average precipitation from 1981 to 2010 is 2.9823236, and the standard deviation for the precipitation from 1981 to 2010 is 7.8180254.
-   From the dataset, we could know that, there is no NA in id and dates. There are 145838 NAs in precipitation, 381221 in snow, 591786 in snwd, 1134358 in tman and 1134420 in tmin. If the distribution of NA is concentrated in one or two variables, the missing data could be an issue; if the NAs are distributed relatively evenly in many variables, the missing data might be an issue.

``` r
ny_noaa %>%
  filter(!is.na(snow)) %>%
  group_by(snow) %>%
    summarise(n_value = n()) %>%
    filter(n_value == max(n_value))
```

    ## # A tibble: 1 x 2
    ##    snow n_value
    ##   <int>   <int>
    ## 1     0 2008508

-   The most observed temperature for snawfall is 0, because 0 is the melting point for ice. It would lasts for a longer time than other temperatures.

``` r
ny_noaa %>%
  filter(month %in% c("01", "07")) %>%
  group_by(year, month, id) %>%
  summarise(average_tmax = mean(tmax, na.rm = TRUE)) %>%
  ggplot(aes(x = year, y = average_tmax, color = id)) +
  geom_line(alpha = .4) +
  labs(
    title = "Average Max Temperature of Janurary and July",
    x = "Year",
    y = "Average Max Temperature (ºC)"
  ) +
  scale_x_continuous(
    breaks = c(1981, 1990, 2000, 2010),
    labels = c("1981", "1990", "2000", "2010")
  ) +
  geom_point(alpha = 0.4) +
  facet_grid(~month) +
  theme_bw() +
  theme(legend.position = "none")
```

    ## Warning: Removed 5640 rows containing missing values (geom_path).

    ## Warning: Removed 5970 rows containing missing values (geom_point).

<img src="p8105_hw3_jt3098_files/figure-markdown_github/two-panel plot for tmax in Janurary and July-1.png" width="100%" />

Comments: From the two-panel plots we could know the data recorded in each station about the everage maximum temeperature in January and July. We could use spaghetti strcture to show the data, because we need to provide year, average tmax for each station. If we use boxplot, we could be able to see outliers but it could not give us information among stations. Therefore, speghetti structure is the best choice (for me). We could also see the outliers that lies out of the clusters.

``` r
temp_dist = 
  ny_noaa %>%
  filter(!is.na(tmin), !is.na(tmax)) %>%
  ggplot(aes(x = tmin, y = tmax)) +
  geom_hex() +
  labs(
    title = "Distribution of tmin and tmax from 1981 to 2010"
  ) +
  theme_bw() 

snow_dist = 
  ny_noaa %>%
  filter(!is.na(snow)) %>%
  filter(snow < 100 & snow > 0) %>%
  mutate(year = as.factor(year)) %>%
  ggplot(aes(x = snow, y = year)) +
  geom_density_ridges() +
  labs(
    title = "Distribution of Snowfall > 0 and Snowfall < 100",
    x = "Snowfall (mm)",
    y = "Year"
  ) +
  theme_bw()

snow_dist + temp_dist
```

    ## Picking joint bandwidth of 3.76

<img src="p8105_hw3_jt3098_files/figure-markdown_github/two-panel plot for tmax and tmin-1.png" width="100%" />

Comments: In general, the distribution of snowfall value in each year has a similar shape with very slight difference. However, in the distribution of tmax and tmin, there are many outliers. The most observed value for tmax is approximately 0 to 30 ºC, while the most observed value for tmin is -15ºC to 15ºC.
